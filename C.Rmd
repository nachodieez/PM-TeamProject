---
title: "Exercise C for Predictive Modeling"
author:
  - Pablo Vidal Fernández	  100483812
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	  100484904
output: pdf_document
urlcolor: magenta
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup general, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
```{r packages C, include = FALSE}
require(tidyverse)
require(pracma)
require(Matrix)
require(FactoMineR)
require(glmnet)
require(corrplot)
require(car)
require(MASS)
require(fastDummies)
require(dplyr)
```
```{r utils and data, include = FALSE}
load("10.RData")
categorical_idx <- names(which(sapply(df, class) == "factor"))
factorial_idx   <- categorical_idx[-1]
numerical_idx   <- setdiff(names(df), categorical_idx)

drop_columns <- function(df, cols) {
  new_df <- subset(df, select = !(names(df) %in% cols))
  categorical_idx <<- names(which(sapply(new_df, class) == "factor"))
  factorial_idx   <<- categorical_idx[-1]
  numerical_idx   <<- setdiff(names(new_df), categorical_idx)
  return(new_df)
}
```

## Introduction

In the search of a dataset of true interest for ourselves, we were
sadly met with failure.
After exhaustive indagations in public repositories, we were
not able to find a suitable one.
We did, nonetheless, stumbled upon intriguing datasets,
which, unfortunately, were time series.

As such, and although it was against our primary desire,
we ended up using the [mysterious.RData](https://github.com/egarpor/handy/blob/master/datasets/mysterious.RData?raw=true).
This, presented some challenges, which will be explored
on the next section, which made it certainly more appealing
than the majority of exploited online datasets.

Let us detail briefly the contents of the selected data.
It is comprised of one binary prediction variable `y` and
903 predictors, seggregated by numericals, 893 of them, and
categoricals, the remaining 10.
It has 224 observations, which already tells us that we will
need to consider dimensional reduction techniques.
It has some NaNs present in it, a total of 81 values
spanning 26 rows.
On the next section we will detail our analysis process
and buildup of the prediction model.

## Model Building

### Preprocessing

We first start by analysing the distributions of NaNs on
the various variables:

```{r nans}
cols_with_na <- sort(sapply(df, function(x) sum(is.na(x))), decreasing = TRUE)
cols_with_na[cols_with_na > 0]
```

We can see that the 81 missing values are within 4 variables.
Three of them are categorical.
Before deciding whether we should impute or not, we want to
be sure if this variables are important for the prediction.

For the numerical variable, a simple t-test is enough to determine
that there is a significant difference between the groups.
Hence, we will keep it and impute it using the median.

```{r t test}
t.test(num.148 ~ y, data = df)
```

The categorical variables we are going to look at them
using a MCA.
We can see that factors 6, 7 and 8 are not close to the other ones.
Considering this, the fact that in the univariate fit
for each of this factors no category has a significative coefficient,
that factors 6 and 7 have a lot of categories,
and that we still have other 900 variables we have decided to drop them.
We will later return to the MCA. [^1]

[^1]: During the whole document, in the codeblocks we will
refer to numerical variables as `numerical_idx`, factorials
as `factorials_idx` and factorials together with the
prediction variable as `categorical_idx`.

```{r full mca, fig.dim = c(8, 4)}
full_mca <- MCA(X = df[categorical_idx], graph = FALSE)
plot.MCA(full_mca, choix = "var", graph.type = "ggplot")
```
```{r impute and drop, echo = FALSE}
df$num.148[is.na(df$num.148)] <- median(df$num.148, na.rm = TRUE)
df <- drop_columns(df, c("fac.6", "fac.7", "fac.8"))
```

For the following part we will need two auxiliary functions.
With the first, we will check whether there is (up to
float accuracy), perfect multicollinearity between some
of our variables.
The second one checks if some column may be considered
as constant (taking in account some variability for
the numerical variables).

```{r preprocessing functions}
find_mulcol <- function(x) {
  # Handle factors
  if (all(sapply(x, class) == "factor")) {
    x <- dummy_cols(x, remove_first_dummy = TRUE,
            remove_selected_columns = TRUE)
  }

  # Fit each column with the rest
  problematicas <- c()
  for (i in seq_len(ncol(x))) {
    f <- lm(x[, i] ~ ., data = x[, -i])
    if (summary(f)$r.squared > 1 - 1e-10) {
      problematicas <- append(problematicas, colnames(x)[i])
    }
  }

  return(problematicas)
}

check_if_constant_column <- function(x) {
  num <- x[1]
  c1  <- sum(x == num) > length(x) / 2
  if (class(x) == "numeric") {
    c2 <- all(near(x, num, tol = 0.001 * mean(x)))
    return(c1 & c2)
  } else {
    return(all(x == num))
  }
}
```

We now use them to remove problematic predictors.

```{r problematic, warning = FALSE}
# Problematic categoricals
find_mulcol(df[factorial_idx])
df <- drop_columns(df, c("fac.9"))
find_mulcol(df[factorial_idx])

# Drop constant columns
constant_columns_idx <- names(which(sapply(df, check_if_constant_column)))
df <- drop_columns(df, constant_columns_idx)
```

### Dimensionality reduction

We had several ideas on how to approach dimensionality reduction.
Mainly, we discussed whether to use PCA or Lasso with the
numerical variables.
Finally, we settled on Lasso as not only did it prove to
be more numerically robust on the particularities of the
dataset, but it also carries information of the prediction
variable for the variable selection.

```{r lasso, fig.dim = c(8, 4)}
y     <- df$y
y_enc <- dplyr::recode(y, A = 0, B = 1)
x_num <- df[numerical_idx]
x_scl <- scale(x_num)
lasso <- glmnet(x_num, y_enc, family = "binomial", alpha = 1)

plot(lasso, xvar = "lambda", label = TRUE)
```

There seem to be some important variables as they converge
slowly to zero when incrementing the value of $\lambda$.
However, we do not know which is the best value for $\lambda$.
We will use cross-validation and the rule of $\hat{\lambda}_{k-1SE}$.
The idea behind it is visually seen in [Figure 4.5](https://bookdown.org/egarpor/PM-UC3M/images/R/lasso-sel.png),
where it is evidenced that $\hat{\lambda}_{k-1SE}$ consistently
predicts the true model for large enough datasets.

Due to cross validation in `glmnet` being random, we perform
10 7-fold [^2] CV Lasso regressions and use the average
obtained $\hat{\lambda}_{k-1SE}$.

[^2]: The number 7 is set because it divides the number
of observations.

```{r lambda_selection}
optimal_lambda <- 0.0
for (i in 1:10) {
  cv_lasso        <- cv.glmnet(x_scl, y_enc, family = "binomial",
                      alpha = 1, nfolds = 7)
  optimal_lambda  <- optimal_lambda + cv_lasso$lambda.1se
}
optimal_lambda    <- optimal_lambda / 10

mod_lasso <- glmnet(x_scl, y_enc, family = "binomial",
              alpha = 1, lambda = optimal_lambda)
pred_nums <- predict(mod_lasso, type = "coefficients",
                      s = optimal_lambda)[-1, ] != 0

x_lasso   <- as.data.frame(x_scl[, pred_nums])
```

The best $\lambda$ is `r round(optimal_lambda, 4)`.
It has `r ncol(x_lasso)` non-null coefficients.

Note that we have only reduced the number of variables.
We still need to deal with multicollinearity between
the selected predictors.
We will do so using PCA, but first we filter out perfect
multicollinearity as they will bring numerical inestability
to the PCA.
We illustrate with a corrplot that there is still huge
correlations

```{r problematic lasso, warning = FALSE, fig.dim = c(16, 16)}
problematic <- find_mulcol(x_lasso)
while (length(problematic) > 0) {
  x_lasso <- subset(x_lasso, select = !(names(x_lasso) %in% problematic[1]))
  problematic <- find_mulcol(x_lasso)
}

corrplot(cor(x_lasso))
```

We can now safely proceed to the PCA.
The next question that arises is how to select
the optimal number of principal components.
We do this by using a geometric procedure.
Let us detail it in the following paragraph.

First we plot the cummulative variance explained
by the PCs.
Note that one could also take the explained variance, not
the cummulative sum.
However this makes for a less numerically stable algorithm
as monotony is not ensured.
Then, we measure the vertical distance from each of the points
to the line which joins the first and last.
Finally, we pick the number of PCs which maximizes
the aforementioned distance.

```{r pca, echo = FALSE, fig.dim = c(8, 4)}
pca <- princomp(x_lasso)

var_pca <- pca$sdev^2
var_pca <- var_pca[var_pca / sum(var_pca) > 0.01]
cum_pca <- cumsum(var_pca)
slp_pca <- (tail(cum_pca, 1) - cum_pca[1]) / (length(cum_pca) - 1)
dst_pca <- cum_pca - slp_pca * seq.int(0, length(cum_pca) - 1)

n_pcs <- which.max(dst_pca)

plot(cum_pca, bg = "black")
lines(cum_pca[1] + slp_pca * seq.int(0, length(cum_pca) - 1), col = "#D81B60")
for (i in seq_len(length(cum_pca))) {
  if (i == n_pcs) {
    lines(c(i, i), c(cum_pca[i], cum_pca[1] + slp_pca * (i - 1)),
          col = "#FFC107")
  } else {
    lines(c(i, i), c(cum_pca[i], cum_pca[1] + slp_pca * (i - 1)),
          col = "#1E88E5")
  }
}

new_x_num <- as.data.frame(pca$scores[, 1:n_pcs])
```

In this case, we find the optimal number of PCs to
be `r n_pcs`.
Now we use the exact same approach with the categorical
variables with a MCA.

```{r mca, echo = FALSE, fig.dim = c(8, 4)}
mca <- MCA(X = df[factorial_idx], graph = FALSE, ncp = 20)

cum_mca <- mca$eig[, 3]
slp_mca <- (tail(cum_mca, 1) - cum_mca[1]) / (length(cum_mca) - 1)
dst_mca <- cum_mca - slp_mca * seq.int(0, length(cum_mca) - 1)

n_mcs <- which.max(dst_mca)

plot(cum_mca, bg = "black")
lines(cum_mca[1] + slp_mca * seq.int(0, length(cum_mca) - 1), col = "#D81B60")
for (i in seq_len(length(cum_mca))) {
  if (i == n_mcs) {
    lines(c(i, i), c(cum_mca[i], cum_mca[1] + slp_mca * (i - 1)),
          col = "#FFC107")
  } else {
    lines(c(i, i), c(cum_mca[i], cum_mca[1] + slp_mca * (i - 1)),
          col = "#1E88E5")
  }
}

new_x_fac <- as.data.frame(mca$ind$coord[, 1:n_mcs])
```

For them we find `r n_mcs` as the optimal number
of MCs to consider.

Let us stop here for a moment to appreciate what we have done.
We came all the way to reduce the number of variables to
just `r n_pcs + n_mcs`.
It is at this point that we consider we have reduced
the dimensionality enough so that we may start to build
the final model.

### Final Model

To end with, we use a simple step BIC [^3] search to find the
final model.

[^3]: We refer to our own simulated results on the
exercise B.6 to prefer the BIC search over AIC search.

```{r step, warning = FALSE}
# y_enc is the encoded prediction variable
# new_x_num are the selected PCAs
# new_x_fac are the selected MCAs
final_df  <- cbind(y_enc, new_x_num, new_x_fac)

mod_zero  <- glm(y_enc ~ 1, family = binomial, data = final_df)
mod_all   <- glm(y_enc ~ ., family = binomial, data = final_df)

both <- MASS::stepAIC(mod_zero, scope = list(lower = mod_zero, upper = mod_all),
                      direction = "both", trace = 0, k = log(nrow(final_df)))
```

The final model is then:
```{r final model, echo = FALSE}
  print(both$formula)
```

To end the project, we report the accuracy obtained
in predicting the objective variable using 7-fold
cross validation.

```{r cv accuracy}
nfolds   <- 7

cross_val <- function(model, df, nfolds) {
  acc       <- 0.0
  fold_size <- floor(nrow(df) / nfolds)
  for (i in 1:nfolds) {
    sr  <- (i - 1) * fold_size
    er  <- i * fold_size
    f   <- glm(model$formula, family = binomial, data = df[-(sr:er), ])
    p   <- predict(f, newdata = df[sr:er, -1], type = "response")
    acc <- acc + mean((p > 0.5) == df[sr:er, 1])
  }
  return(acc / nfolds)
}

acc <- cross_val(both, final_df, nfolds)
```

The reported 7-fold accuracy is `r round(acc, 4)`.


## Conclussions

Working with [mysterious.RData](https://github.com/egarpor/handy/blob/master/datasets/mysterious.RData?raw=true) has been a real challenge.
The number of variables was much higher than the number of instances. This the presence of multicollinearity among the variables has caused numerous problems, as has been shown throughout the report.

Despite the difficulties encountered, and thanks to the use of variable selection and dimensionality reduction techniques, it has been possible to obtain a model that, apparently, has great predictive power, with an estimated accuracy of 0.9435 (obtained through cross validation).

However, the interpretability of the model may have been affected by these techniques since when performing regression on the principal components, the interpretability of a coefficient linked to a principal component depends on the component itself.
Nevertheless, the principal components may have clear interpretations and, in that case, the model would not only be highly accurate but also easily interpretable, making it a very useful model.
Unfortunately, it is difficult to determine whether the latter is true in this case, since you we don't know what each of the variables refers to, beyond its value.

## References

García-Portugués, E. (2022). Notes for Predictive Modeling. Version 5.9.10. ISBN 978-84-09-29679-8. Available at https://bookdown.org/egarpor/PM-UC3M/.
