---
title: "Exercise C for Predictive Modeling"
author:
  - Pablo Vidal Fernández	  100483812
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	  100484904
output: pdf_document
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
require(tidyverse)
require(pracma)
require(Matrix)
require(FactoMineR)
require(glmnet)
require(corrplot)
require(car)
require(MASS)
```

## Introduction

In the search of a dataset of true interest for ourselves, we were
sadly met with failure.
After exhaustive indagations in public repositories, we were
not able to find a suitable one.
We did, nonetheless, stumbled upon intriguing datasets,
which, unfortunately, were time series.

As such, and although it was against our primary desire,
we ended up using the [mysterious.RData](https://github.com/egarpor/handy/blob/master/datasets/mysterious.RData?raw=true).
This, presented some challenges, which will be explored
on the next section, which made it certainly more appealing
than the majority of exploited online datasets.

Let us detail briefly the contents of the selected data.
It is comprised of one binary prediction variable `y` and
903 predictors, seggregated by numericals, 893 of them, and
categoricals, the remaining 10.
It has 224 observations, which already tells us that we will
need to consider dimensional reduction techniques.
It has some NaNs present in it, a total of 81 values
spanning 26 rows.
On the next section we will detail our analysis process
and buildup of the prediction model.


```{r}
load("mysterious.RData")

dim(df)

sum(is.na(df))

cols_with_na <- sort(sapply(df, function(x) sum(is.na(x))), decreasing = TRUE)
cols_with_na[cols_with_na > 0]
```

We can see that the 81 missing values are within 4 variables.

```{r}
categorical_idx <- names(which(sapply(df, class) == "factor"))
factorial_idx   <- categorical_idx[-1]
numerical_idx   <- setdiff(names(df), categorical_idx)

mca_fit <- MCA(X = df[categorical_idx], graph = FALSE)
plot.MCA(mca_fit, choix = "var", graph.type = "ggplot")
```

```{r internal fit discarded fac, echo = F, results = "hide"}
fit6 <- glm(y ~ fac.6, family = binomial, data = df)
fit7 <- glm(y ~ fac.7, family = binomial, data = df)
fit8 <- glm(y ~ fac.8, family = binomial, data = df)
```
```{r t-test num 148}
t.test(df$num.148[df$y == "A"], df$num.148[df$y == "B"])$p.value
```

We can see that factors 6, 7 and 8 are not close to the other ones. Considering this, the fact that in the univariate fit for each of this factors no category has a significative coefficient, that factors 6 and 7 have a lot of categories, and that we still have other 900 variables we have decided to drop them. 

Regarding the numerical variable, `num.148`, we have seen that there is a significant difference between groups, so we will keep it and impute the median. 

```{r preprocessing functions}
donde_esta <- function(x){
  if (prod(sapply(x, class) == "factor")){
    x <- model.matrix(~ . - 1, data = x)
    x <- as.data.frame(x)
  }
  problematicas <- c()
  for (i in 1:ncol(x)){
    aaa <- x[,i]
    bbb <- x[,-i]
    fff <- lm(aaa ~ ., data = bbb)
    if (summary(fff)$r.squared > 0.99){
      problematicas <- append(problematicas, colnames(x)[i])
      z <<- fff
    }
  }
  
  return(problematicas)
}

check_if_constant_column <- function(x){
  num <- x[1]
  c1 <- sum(x == num) > length(x)/2
  if(class(x) == "numeric"){
    c2 <- all(near(x, num, tol = 0.001))
    return (c1 & c2)
  }
  else{
    return(all(x == num))
  }  
}

drop_columns <- function(df, cols) {
  new_df <- subset(df, select = !(names(df) %in% cols))
  categorical_idx <<- names(which(sapply(new_df, class) == "factor"))
  factorial_idx   <<- categorical_idx[-1]
  numerical_idx   <<- setdiff(names(new_df), categorical_idx)
  return(new_df)
}
```

```{r preprocessing}
df <- drop_columns(df, c("fac.6", "fac.7", "fac.8"))

# fac.9 and fac.10 R^2 == 1 (use donde_esta)
df <- drop_columns(df, c("fac.9"))

df$num.148[is.na(df$num.148)] <- median(df$num.148, na.rm = T)

# Drop constant columns
constant_columns_idx <- which(sapply(df, check_if_constant_column))
df <- drop_columns(df, constant_columns_idx)

y <- df$y
```
Once preprocessing is done, we will do a preliminary Lasso model to try to decrease the number of predictors:

```{r lasso}
x1 <- scale(df[numerical_idx])
fit2 <- glmnet(x1, df$y, family = "binomial", alpha = 1)
plot(fit2)
```

```{r plot_lasso2}
plot(fit2, xvar = "lambda", label = T)
```

There seem to be some important variables. However, we don't know which is the best value for lambda. We will use cross-validation and the rule of $\hat{\lambda}_{k-1SE}$.

```{r lambda_selection}
optimal_lambda <- 0.0
for (i in 1:10) {
  kcvLasso <- cv.glmnet(x = x1, y = y, family = "binomial", alpha = 1,
                      type.measure = "deviance")
  optimal_lambda <- optimal_lambda + kcvLasso$lambda.1se
}
optimal_lambda <- optimal_lambda/10

print(optimal_lambda)
indMin <- which.min(kcvLasso$cvm)
plot(kcvLasso)
abline(h = kcvLasso$cvm[indMin] + c(0, kcvLasso$cvsd[indMin]))
```

The best $\lambda$ is `r round(optimal_lambda, 4)`. It has around 50 non-null coefficients. 

```{r}
modLassoCV <- glmnet(x1, y, family = "binomial", alpha = 1,
                     lambda = optimal_lambda)

selPreds <- predict(modLassoCV, type = "coefficients",
                    s = optimal_lambda)[-1, ] != 0
x1_2 <- as.data.frame(x1[, selPreds])

problematic <- donde_esta(x1_2)
while (length(problematic) > 0) {
  x1_2 <- subset(x1_2, select = !(names(x1_2) %in% problematic[1]))
  problematic <- donde_esta(x1_2)
}

corrplot(cor(x1_2))
```

As there are still correlated features, we decide to perform a PCA:

```{r}
fit_pca <- princomp(x1_2)

v <- fit_pca$sdev^2
v <- v[v/sum(v) > 0.01]
c <- cumsum(v)
m <- (tail(c,1) - c[1])/(length(c) - 1)
d <- c - m*c(0:(length(c)-1)) + c[1]

plot(d)
n_pcs <- which.max(d)
new_x <- as.data.frame(fit_pca$scores[,1:n_pcs])
```

```{r}
mca_fit <- MCA(X = df[factorial_idx], graph = FALSE, ncp = 20)
v <- mca_fit$eig[,2]
c <- cumsum(v)
m <- (tail(c,1) - c[1])/(length(c) - 1)
d <- c - m*c(0:(length(c)-1)) + c[1]

plot(d)
n_mcs <- which.max(d)
new_f <- as.data.frame(mca_fit$ind$coord[,1:n_mcs])
```


```{r}
x_tot <- cbind(new_x, new_f)
fit5 <- glm(y ~ ., family = binomial, data = x_tot)
summary(fit5)

modZero <- glm(y ~ 1, family = binomial, data = x_tot)
modAll  <- glm(y ~ ., family = binomial, data = x_tot)

both <- MASS::stepAIC(modZero, scope = list(lower = modZero, upper = modAll),
                      direction = "both", trace = 0, k = log(nrow(x_tot)))
```

```{r}
modZero <- glm(y ~ 1, family = binomial, data = df[factorial_idx])
modAll  <- glm(y ~ ., family = binomial, data = df[factorial_idx])

bothfac <- MASS::stepAIC(modZero, scope = list(lower = modZero, upper = modAll),
                      direction = "both", trace = 0, k = log(nrow(df)))
```

```{r}
new_x2 <- cbind(new_x, df[factorial_idx])

modZero <- glm(y2[1:175] ~ 1, family = binomial, data = new_x2[1:175,])
modAll  <- glm(y2[1:175] ~ ., family = binomial, data = new_x2[1:175,])

both <- MASS::stepAIC(modZero, scope = list(lower = modZero, upper = modAll),
                      direction = "both", trace = 0, k = log(175))
```

```{r}
y_num  <- dplyr::recode(y, A = 0, B = 1)
nfolds <- 7

cross_val <- function(model, y, df, nfolds) {
  acc <- 0.0
  df_tot <- cbind(y, df)
  fold_size <- floor(nrow(df)/nfolds)
  for (i in 1:nfolds) {
    sr  <- (i - 1)*fold_size
    er  <- i*fold_size
    f   <- glm(model$formula, family = binomial, data = df_tot[-(sr:er),])
    p   <- predict(f, newdata = df_tot[sr:er,-1], type = "response")
    acc <- acc + mean((p > 0.5) == y[sr:er])
  }
  return (acc / nfolds)
}

acc <- cross_val(both, y_num, x_tot, nfolds)
```

```{r}
require(dplyr)

y2 <- dplyr::recode(y, A = 0, B = 1)

k <- 1:16
mse <- c()
for (i in 0:13){
  f <- glm(y[-(i*16+k)] ~ ., family = binomial, data = new_x[-(i*16+k),])
  pred <- predict(f, newdata = new_x[(i*16+k),], type = "response") 
  mse <- append(mse, mean((y2[i*16+k] - pred)^2))
  print(table(y2[i*16+k], pred > 0.5))
}

```


Esto es una prueba (MUY chapucera) para ver dónde carajo hay multicolinealidad:

```{r}
donde_esta <- function(x){
  problematicas <- c()
  for (i in 1:ncol(x)){
    aaa <- x[,i]
    bbb <- x[,-i]
    fff <- lm(aaa ~ ., data = bbb)
    if (summary(fff)$r.squared == 1){
      print(i)
      problematicas <- append(problematicas, colnames(x)[i])
    }
  }
  
  return(problematicas)
}
```

```{r}
modAll2_cat <- glm(y ~ ., family = binomial, data = x_categorical)

x_categorical$y <- y
z <- model.matrix(y ~ ., data = x_categorical) 
donde_esta(as.data.frame(z))

#con esto queda demostrado que la 10 está completamente determinada por la 9
a <- glm(fac.10b1 ~ fac.9b1 + fac.9c1 + fac.9d1 + fac.9e1 + fac.9f1, family = binomial, data = as.data.frame(z))
b <- glm(fac.10c1 ~ fac.9b1 + fac.9c1 + fac.9d1 + fac.9e1 + fac.9f1, family = binomial, data = as.data.frame(z))
estupido <- "fac.10"
estupido_idx <- which(colnames(x_categorical) == estupido)

x_categorical <- x_categorical[, -estupido_idx]

modAll_cat2 <- glm(y ~ ., family = binomial, data = x_categorical)
```

